#!/usr/bin/env python3
"""
ä¿®å¤crawleré€‰æ‹©å™¨ï¼Œé€‚é…äººæ°‘ç½‘æ–°çš„ç½‘é¡µç»“æ„
"""

import requests
from bs4 import BeautifulSoup


def analyze_people_website():
    """åˆ†æäººæ°‘ç½‘ç½‘ç«™ç»“æ„"""
    print("ğŸ” åˆ†æäººæ°‘ç½‘ç½‘ç«™ç»“æ„...")
    print("=" * 50)
    
    # æµ‹è¯•ä¸€ä¸ªå…·ä½“çš„æ–‡ç« é¡µé¢
    test_url = "http://politics.people.com.cn/n1/2025/0908/c1024-40559264.html"
    
    try:
        response = requests.get(test_url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            print("åˆ†æé¡µé¢ç»“æ„...")
            
            # æŸ¥æ‰¾æ‰€æœ‰divå…ƒç´ åŠå…¶class
            divs = soup.find_all('div', class_=True)
            print(f"æ‰¾åˆ° {len(divs)} ä¸ªå¸¦classçš„divå…ƒç´ ")
            
            # åˆ†æå¯èƒ½çš„æ–‡ç« å†…å®¹åŒºåŸŸ
            content_candidates = []
            
            for div in divs:
                class_name = ' '.join(div.get('class', []))
                text_length = len(div.get_text(strip=True))
                
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ–‡ç« å†…å®¹çš„åŒºåŸŸ
                if (text_length > 500 and 
                    any(keyword in class_name.lower() for keyword in ['content', 'article', 'text', 'main', 'body'])):
                    content_candidates.append({
                        'class': class_name,
                        'text_length': text_length,
                        'element': div
                    })
            
            print(f"æ‰¾åˆ° {len(content_candidates)} ä¸ªå¯èƒ½çš„å†…å®¹åŒºåŸŸ:")
            for i, candidate in enumerate(content_candidates[:5]):
                print(f"  {i+1}. class=\"{candidate['class']}\" (é•¿åº¦: {candidate['text_length']})")
                
                # æ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
                text_preview = candidate['element'].get_text(strip=True)[:100]
                print(f"     é¢„è§ˆ: {text_preview}...")
                print()
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            print("å»ºè®®çš„é€‰æ‹©å™¨:")
            selectors = []
            
            for div in divs:
                class_name = ' '.join(div.get('class', []))
                if any(keyword in class_name.lower() for keyword in ['content', 'article', 'text', 'main', 'body']):
                    selector = f".{class_name.replace(' ', '.')}"
                    selectors.append(selector)
            
            # å»é‡å¹¶æ’åº
            unique_selectors = list(set(selectors))
            unique_selectors.sort()
            
            for selector in unique_selectors[:10]:
                print(f"  {selector}")
            
            return unique_selectors
            
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        return []


def test_selectors():
    """æµ‹è¯•é€‰æ‹©å™¨æ•ˆæœ"""
    print("\nğŸ§ª æµ‹è¯•é€‰æ‹©å™¨æ•ˆæœ...")
    print("=" * 50)
    
    test_url = "http://politics.people.com.cn/n1/2025/0908/c1024-40559264.html"
    
    # æ–°çš„é€‰æ‹©å™¨åˆ—è¡¨
    new_selectors = [
        '.rm_txt_con',
        '.content',
        '.article-content',
        '#content',
        '.main-content',
        '.text-content',
        '.article-body',
        '.post-content',
        '.entry-content',
        '.article-text',
        '.content-text',
        '.main-text',
        '.body-text',
        '.article-main',
        '.content-main',
        '.text-main',
        '.main-body',
        '.content-body',
        '.text-body',
        '.article-content-text',
        '.content-article',
        '.text-article',
        '.main-article',
        '.body-content',
        '.body-article',
        '.body-text',
        '.article',
        '.post',
        '.entry',
        '.text',
        '.body',
        '.main',
        '.content'
    ]
    
    try:
        response = requests.get(test_url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            print("æµ‹è¯•é€‰æ‹©å™¨:")
            working_selectors = []
            
            for selector in new_selectors:
                try:
                    elements = soup.select(selector)
                    if elements:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®é™…å†…å®¹
                        total_text = sum(len(elem.get_text(strip=True)) for elem in elements)
                        if total_text > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                            working_selectors.append({
                                'selector': selector,
                                'count': len(elements),
                                'text_length': total_text
                            })
                            print(f"  âœ… {selector}: {len(elements)} ä¸ªå…ƒç´ , {total_text} å­—ç¬¦")
                        else:
                            print(f"  âš ï¸ {selector}: {len(elements)} ä¸ªå…ƒç´ , ä½†å†…å®¹å¤ªå°‘ ({total_text} å­—ç¬¦)")
                    else:
                        print(f"  âŒ {selector}: æœªæ‰¾åˆ°å…ƒç´ ")
                except Exception as e:
                    print(f"  âŒ {selector}: é”™è¯¯ - {e}")
            
            print(f"\næ‰¾åˆ° {len(working_selectors)} ä¸ªæœ‰æ•ˆçš„é€‰æ‹©å™¨")
            
            # æ¨èæœ€ä½³é€‰æ‹©å™¨
            if working_selectors:
                best_selector = max(working_selectors, key=lambda x: x['text_length'])
                print(f"æ¨èä½¿ç”¨: {best_selector['selector']}")
                return best_selector['selector']
            
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
    
    return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿®å¤crawleré€‰æ‹©å™¨...")
    print("=" * 60)
    
    # åˆ†æç½‘ç«™ç»“æ„
    selectors = analyze_people_website()
    
    # æµ‹è¯•é€‰æ‹©å™¨
    best_selector = test_selectors()
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“‹ ä¿®å¤å»ºè®®:")
    print("   1. æ›´æ–° crawler/services.py ä¸­çš„ _extract_content æ–¹æ³•")
    print("   2. æ·»åŠ æ–°çš„é€‰æ‹©å™¨åˆ° content_selectors åˆ—è¡¨")
    print("   3. æµ‹è¯•æ–°çš„é€‰æ‹©å™¨æ˜¯å¦æœ‰æ•ˆ")
    
    if best_selector:
        print(f"\nğŸ’¡ æ¨èçš„é€‰æ‹©å™¨: {best_selector}")
        print("   å¯ä»¥å°†æ­¤é€‰æ‹©å™¨æ·»åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­")


if __name__ == '__main__':
    main()
