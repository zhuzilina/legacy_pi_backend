#!/usr/bin/env python3
"""
è¯Šæ–­crawlerçˆ¬å–é—®é¢˜
"""

import os
import sys
import django
import requests
import time
from datetime import datetime

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'legacy_pi_backend.settings')
django.setup()

from crawler.services import PeopleNetCrawler
from crawler.redis_service import redis_service
from crawler.redis_models import RedisNewsArticle, RedisCrawlTask


def test_network_connectivity():
    """æµ‹è¯•ç½‘ç»œè¿æ¥"""
    print("ğŸŒ æµ‹è¯•ç½‘ç»œè¿æ¥...")
    print("=" * 50)
    
    test_urls = [
        "http://www.people.com.cn",
        "http://www.people.com.cn/GB/59476/",
        "http://www.people.com.cn/GB/",
    ]
    
    for url in test_urls:
        try:
            print(f"æµ‹è¯• {url}...")
            response = requests.get(url, timeout=10)
            print(f"   çŠ¶æ€ç : {response.status_code}")
            print(f"   å†…å®¹é•¿åº¦: {len(response.text)}")
            print(f"   ç¼–ç : {response.encoding}")
            
            if response.status_code == 200:
                print("   âœ… è¿æ¥æˆåŠŸ")
            else:
                print(f"   âŒ è¿æ¥å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ è¿æ¥å¼‚å¸¸: {e}")
        print()


def test_crawler_service():
    """æµ‹è¯•çˆ¬è™«æœåŠ¡"""
    print("ğŸ•·ï¸ æµ‹è¯•çˆ¬è™«æœåŠ¡...")
    print("=" * 50)
    
    try:
        crawler = PeopleNetCrawler()
        print("âœ… çˆ¬è™«æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•è·å–æ–°é—»é“¾æ¥
        print("æµ‹è¯•è·å–æ–°é—»é“¾æ¥...")
        news_links = crawler._get_news_from_homepage()
        print(f"   é¦–é¡µè·å–ç»“æœ: {len(news_links)} æ¡é“¾æ¥")
        
        if not news_links:
            print("   å°è¯•ç›´æ¥é¡µé¢è·å–...")
            news_links = crawler._get_news_from_direct_page()
            print(f"   ç›´æ¥é¡µé¢è·å–ç»“æœ: {len(news_links)} æ¡é“¾æ¥")
        
        if news_links:
            print("   âœ… æˆåŠŸè·å–æ–°é—»é“¾æ¥")
            print(f"   å‰3æ¡é“¾æ¥:")
            for i, link in enumerate(news_links[:3]):
                print(f"     {i+1}. {link.get('title', 'No title')[:50]}...")
        else:
            print("   âŒ æ— æ³•è·å–æ–°é—»é“¾æ¥")
            
    except Exception as e:
        print(f"âŒ çˆ¬è™«æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
    print()


def test_redis_connection():
    """æµ‹è¯•Redisè¿æ¥"""
    print("ğŸ”´ æµ‹è¯•Redisè¿æ¥...")
    print("=" * 50)
    
    try:
        if redis_service.test_connection():
            print("âœ… Redisè¿æ¥æ­£å¸¸")
            
            # æµ‹è¯•åŸºæœ¬æ“ä½œ
            test_key = "test_crawler_diagnosis"
            redis_service.redis_client.set(test_key, "test_value", ex=60)
            value = redis_service.redis_client.get(test_key)
            if value == "test_value":
                print("âœ… Redisè¯»å†™æ­£å¸¸")
                redis_service.redis_client.delete(test_key)
            else:
                print("âŒ Redisè¯»å†™å¼‚å¸¸")
        else:
            print("âŒ Redisè¿æ¥å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ Redisæµ‹è¯•å¤±è´¥: {e}")
    print()


def test_crawler_api():
    """æµ‹è¯•çˆ¬è™«API"""
    print("ğŸ”Œ æµ‹è¯•çˆ¬è™«API...")
    print("=" * 50)
    
    base_url = "http://localhost"
    
    # 1. æµ‹è¯•é‡ç½®API
    print("1. æµ‹è¯•é‡ç½®API:")
    try:
        response = requests.post(f"{base_url}/api/crawler/reset/")
        print(f"   çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"   å“åº”: {data.get('message', 'No message')}")
            print("   âœ… é‡ç½®APIæ­£å¸¸")
        else:
            print(f"   âŒ é‡ç½®APIå¤±è´¥: {response.text}")
    except Exception as e:
        print(f"   âŒ é‡ç½®APIå¼‚å¸¸: {e}")
    print()
    
    # 2. æµ‹è¯•æ¯æ—¥æ–‡ç« API
    print("2. æµ‹è¯•æ¯æ—¥æ–‡ç« API:")
    try:
        response = requests.get(f"{base_url}/api/crawler/daily/")
        print(f"   çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"   çŠ¶æ€: {data.get('status', 'Unknown')}")
            print(f"   æ¶ˆæ¯: {data.get('message', 'No message')}")
            
            if data.get('status') == 'crawling':
                print("   âœ… çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨")
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åæ£€æŸ¥ç»“æœ
                print("   ç­‰å¾…30ç§’åæ£€æŸ¥ç»“æœ...")
                time.sleep(30)
                
                response2 = requests.get(f"{base_url}/api/crawler/daily/")
                if response2.status_code == 200:
                    data2 = response2.json()
                    print(f"   ç¬¬äºŒæ¬¡çŠ¶æ€: {data2.get('status', 'Unknown')}")
                    print(f"   æ–‡ç« æ•°é‡: {data2.get('total_articles', 0)}")
                    
                    if data2.get('status') == 'cached' and data2.get('total_articles', 0) > 0:
                        print("   âœ… çˆ¬å–æˆåŠŸï¼Œæœ‰æ•°æ®è¿”å›")
                    elif data2.get('status') == 'crawling':
                        print("   â³ çˆ¬å–ä»åœ¨è¿›è¡Œä¸­")
                    else:
                        print(f"   âš ï¸ çˆ¬å–å¯èƒ½å¤±è´¥: {data2}")
                        
            elif data.get('status') == 'cached':
                print("   âœ… è¿”å›ç¼“å­˜æ•°æ®")
            else:
                print(f"   âš ï¸ æœªçŸ¥çŠ¶æ€: {data.get('status')}")
        else:
            print(f"   âŒ æ¯æ—¥æ–‡ç« APIå¤±è´¥: {response.text}")
    except Exception as e:
        print(f"   âŒ æ¯æ—¥æ–‡ç« APIå¼‚å¸¸: {e}")
    print()


def test_article_crawling():
    """æµ‹è¯•æ–‡ç« çˆ¬å–"""
    print("ğŸ“° æµ‹è¯•æ–‡ç« çˆ¬å–...")
    print("=" * 50)
    
    try:
        crawler = PeopleNetCrawler()
        
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•é“¾æ¥
        test_link = {
            'title': 'æµ‹è¯•æ–‡ç« ',
            'url': 'http://www.people.com.cn/GB/59476/',
            'source': 'äººæ°‘ç½‘'
        }
        
        print("æµ‹è¯•çˆ¬å–æ–‡ç« è¯¦æƒ…...")
        article_data = crawler._crawl_article_detail(test_link)
        
        if article_data:
            print("   âœ… æ–‡ç« çˆ¬å–æˆåŠŸ")
            print(f"   æ ‡é¢˜: {article_data.get('title', 'No title')}")
            print(f"   å†…å®¹é•¿åº¦: {len(article_data.get('content', ''))}")
            print(f"   å­—æ•°: {article_data.get('word_count', 0)}")
        else:
            print("   âŒ æ–‡ç« çˆ¬å–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
    print()


def check_existing_data():
    """æ£€æŸ¥ç°æœ‰æ•°æ®"""
    print("ğŸ“Š æ£€æŸ¥ç°æœ‰æ•°æ®...")
    print("=" * 50)
    
    try:
        # æ£€æŸ¥æ–‡ç« æ•°æ®
        articles = RedisNewsArticle.filter()
        print(f"æ€»æ–‡ç« æ•°é‡: {len(articles)}")
        
        if articles:
            # æŒ‰çŠ¶æ€åˆ†ç»„
            status_count = {}
            for article in articles:
                status = article.crawl_status
                status_count[status] = status_count.get(status, 0) + 1
            
            print("æŒ‰çŠ¶æ€åˆ†ç»„:")
            for status, count in status_count.items():
                print(f"  {status}: {count}")
            
            # æ˜¾ç¤ºæœ€è¿‘çš„æ–‡ç« 
            print("æœ€è¿‘çš„æ–‡ç« :")
            for i, article in enumerate(articles[:3]):
                print(f"  {i+1}. {article.title[:50]}... ({article.crawl_status})")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« æ•°æ®")
            
        # æ£€æŸ¥ä»»åŠ¡æ•°æ®
        tasks = RedisCrawlTask.filter()
        print(f"æ€»ä»»åŠ¡æ•°é‡: {len(tasks)}")
        
        if tasks:
            recent_task = tasks[0]
            print(f"æœ€è¿‘ä»»åŠ¡: {recent_task.task_name}")
            print(f"ä»»åŠ¡çŠ¶æ€: {recent_task.status}")
            print(f"åˆ›å»ºæ—¶é—´: {recent_task.created_at}")
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥æ•°æ®å¤±è´¥: {e}")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯Šæ–­crawlerçˆ¬å–é—®é¢˜...")
    print("=" * 60)
    
    # 1. æµ‹è¯•ç½‘ç»œè¿æ¥
    test_network_connectivity()
    
    # 2. æµ‹è¯•Redisè¿æ¥
    test_redis_connection()
    
    # 3. æµ‹è¯•çˆ¬è™«æœåŠ¡
    test_crawler_service()
    
    # 4. æµ‹è¯•æ–‡ç« çˆ¬å–
    test_article_crawling()
    
    # 5. æ£€æŸ¥ç°æœ‰æ•°æ®
    check_existing_data()
    
    # 6. æµ‹è¯•API
    test_crawler_api()
    
    print("âœ… è¯Šæ–­å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“‹ è¯Šæ–­æ€»ç»“:")
    print("   - å¦‚æœç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ£€æŸ¥ç½‘ç»œå’Œé˜²ç«å¢™è®¾ç½®")
    print("   - å¦‚æœRedisè¿æ¥å¤±è´¥ï¼Œæ£€æŸ¥RedisæœåŠ¡çŠ¶æ€")
    print("   - å¦‚æœçˆ¬è™«æœåŠ¡å¤±è´¥ï¼Œæ£€æŸ¥ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
    print("   - å¦‚æœAPIæµ‹è¯•å¤±è´¥ï¼Œæ£€æŸ¥DjangoæœåŠ¡çŠ¶æ€")
    print()
    print("ğŸ’¡ å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ:")
    print("   1. ç½‘ç»œé—®é¢˜: æ£€æŸ¥é˜²ç«å¢™å’Œä»£ç†è®¾ç½®")
    print("   2. Redisé—®é¢˜: é‡å¯RedisæœåŠ¡")
    print("   3. çˆ¬è™«é—®é¢˜: æ£€æŸ¥ç›®æ ‡ç½‘ç«™ç»“æ„å˜åŒ–")
    print("   4. APIé—®é¢˜: é‡å¯DjangoæœåŠ¡")


if __name__ == '__main__':
    main()
