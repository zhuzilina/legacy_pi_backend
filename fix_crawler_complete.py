#!/usr/bin/env python3
"""
å®Œæ•´ä¿®å¤crawleré—®é¢˜
"""

import os
import sys
import django
import requests
import time

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'legacy_pi_backend.settings')
django.setup()

from crawler.services import PeopleNetCrawler
from crawler.redis_service import redis_service
from crawler.redis_models import RedisNewsArticle, RedisCrawlTask


def test_redis_connection():
    """æµ‹è¯•Redisè¿æ¥"""
    print("ğŸ”´ æµ‹è¯•Redisè¿æ¥...")
    try:
        if redis_service.test_connection():
            print("âœ… Redisè¿æ¥æ­£å¸¸")
            return True
        else:
            print("âŒ Redisè¿æ¥å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ Redisè¿æ¥å¼‚å¸¸: {e}")
        return False


def test_crawler_functionality():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    print("\nğŸ•·ï¸ æµ‹è¯•çˆ¬è™«åŠŸèƒ½...")
    try:
        crawler = PeopleNetCrawler()
        
        # æµ‹è¯•è·å–æ–°é—»é“¾æ¥
        print("1. æµ‹è¯•è·å–æ–°é—»é“¾æ¥:")
        news_links = crawler._get_news_from_direct_page()
        print(f"   è·å–åˆ° {len(news_links)} æ¡æ–°é—»é“¾æ¥")
        
        if news_links:
            # æµ‹è¯•çˆ¬å–ç¬¬ä¸€ç¯‡æ–‡ç« 
            print("2. æµ‹è¯•çˆ¬å–ç¬¬ä¸€ç¯‡æ–‡ç« :")
            first_link = news_links[0]
            print(f"   æ ‡é¢˜: {first_link.get('title', 'No title')}")
            
            article_data = crawler._crawl_article_detail(first_link)
            if article_data:
                print("   âœ… æ–‡ç« çˆ¬å–æˆåŠŸ")
                print(f"   å†…å®¹é•¿åº¦: {len(article_data.get('content', ''))}")
                print(f"   å­—æ•°: {article_data.get('word_count', 0)}")
                return True
            else:
                print("   âŒ æ–‡ç« çˆ¬å–å¤±è´¥")
                return False
        else:
            print("   âŒ æ— æ³•è·å–æ–°é—»é“¾æ¥")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_save_article():
    """æµ‹è¯•ä¿å­˜æ–‡ç« """
    print("\nğŸ’¾ æµ‹è¯•ä¿å­˜æ–‡ç« ...")
    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ç« æ•°æ®
        test_article = {
            'title': 'æµ‹è¯•æ–‡ç« æ ‡é¢˜',
            'content': 'è¿™æ˜¯ä¸€ç¯‡æµ‹è¯•æ–‡ç« çš„å†…å®¹ã€‚' * 50,  # ç”Ÿæˆè¶³å¤Ÿé•¿çš„å†…å®¹
            'url': 'http://test.example.com',
            'source': 'æµ‹è¯•æ¥æº',
            'category': 'test',
            'word_count': 1000,
            'image_count': 0,
            'image_mapping': {}
        }
        
        # ä¿å­˜æ–‡ç« 
        article = RedisNewsArticle(**test_article)
        article.save()
        
        print("   âœ… æ–‡ç« ä¿å­˜æˆåŠŸ")
        
        # éªŒè¯ä¿å­˜
        saved_article = RedisNewsArticle.get(article.id)
        if saved_article:
            print("   âœ… æ–‡ç« è¯»å–æˆåŠŸ")
            return True
        else:
            print("   âŒ æ–‡ç« è¯»å–å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ç« æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_full_crawling():
    """æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹"""
    print("\nğŸš€ æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹...")
    try:
        crawler = PeopleNetCrawler()
        
        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
        task = RedisCrawlTask.create(
            task_name='æµ‹è¯•çˆ¬å–ä»»åŠ¡',
            target_url='http://www.people.com.cn/GB/59476/',
            task_type='test'
        )
        
        print(f"   åˆ›å»ºä»»åŠ¡: {task.id}")
        
        # æ‰§è¡Œçˆ¬å–
        result = crawler.crawl_today_news(task_id=task.id)
        
        if result and result.get('success'):
            print("   âœ… å®Œæ•´çˆ¬å–æˆåŠŸ")
            print(f"   æˆåŠŸ: {result.get('success_count', 0)} ç¯‡")
            print(f"   å¤±è´¥: {result.get('failed_count', 0)} ç¯‡")
            return True
        else:
            print("   âŒ å®Œæ•´çˆ¬å–å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ å®Œæ•´çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´ä¿®å¤crawleré—®é¢˜...")
    print("=" * 60)
    
    # 1. æµ‹è¯•Redisè¿æ¥
    redis_ok = test_redis_connection()
    
    # 2. æµ‹è¯•çˆ¬è™«åŠŸèƒ½
    crawler_ok = test_crawler_functionality()
    
    # 3. æµ‹è¯•ä¿å­˜æ–‡ç« 
    save_ok = test_save_article()
    
    # 4. æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹
    if redis_ok and crawler_ok and save_ok:
        full_ok = test_full_crawling()
    else:
        full_ok = False
    
    print("\nâœ… ä¿®å¤æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœ:")
    print(f"   Redisè¿æ¥: {'âœ…' if redis_ok else 'âŒ'}")
    print(f"   çˆ¬è™«åŠŸèƒ½: {'âœ…' if crawler_ok else 'âŒ'}")
    print(f"   ä¿å­˜æ–‡ç« : {'âœ…' if save_ok else 'âŒ'}")
    print(f"   å®Œæ•´æµç¨‹: {'âœ…' if full_ok else 'âŒ'}")
    
    if all([redis_ok, crawler_ok, save_ok, full_ok]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼crawlerå·²ä¿®å¤")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤")
        
        if not redis_ok:
            print("   - æ£€æŸ¥Redisé…ç½®å’Œè¿æ¥")
        if not crawler_ok:
            print("   - æ£€æŸ¥çˆ¬è™«é€‰æ‹©å™¨å’Œç½‘é¡µç»“æ„")
        if not save_ok:
            print("   - æ£€æŸ¥Redisæ¨¡å‹å’Œä¿å­˜é€»è¾‘")
        if not full_ok:
            print("   - æ£€æŸ¥å®Œæ•´çˆ¬å–æµç¨‹")


if __name__ == '__main__':
    main()
