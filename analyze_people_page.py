#!/usr/bin/env python3
"""
åˆ†æäººæ°‘ç½‘é¡µé¢ç»“æ„
"""

import requests
from bs4 import BeautifulSoup


def analyze_page_structure(url):
    """åˆ†æé¡µé¢ç»“æ„"""
    print(f"ğŸ” åˆ†æé¡µé¢: {url}")
    print("=" * 50)
    
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ–‡ç« å†…å®¹çš„å…ƒç´ 
            print("æŸ¥æ‰¾æ–‡ç« å†…å®¹å…ƒç´ :")
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰divå…ƒç´ 
            divs = soup.find_all('div')
            print(f"æ€»divæ•°é‡: {len(divs)}")
            
            # 2. æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„div
            content_divs = []
            for div in divs:
                text = div.get_text(strip=True)
                if len(text) > 200:  # è‡³å°‘200ä¸ªå­—ç¬¦
                    class_name = ' '.join(div.get('class', []))
                    content_divs.append({
                        'element': div,
                        'text_length': len(text),
                        'class': class_name,
                        'text_preview': text[:100]
                    })
            
            # æŒ‰æ–‡æœ¬é•¿åº¦æ’åº
            content_divs.sort(key=lambda x: x['text_length'], reverse=True)
            
            print(f"æ‰¾åˆ° {len(content_divs)} ä¸ªåŒ…å«å¤§é‡æ–‡æœ¬çš„div:")
            for i, div_info in enumerate(content_divs[:5]):
                print(f"  {i+1}. class=\"{div_info['class']}\" (é•¿åº¦: {div_info['text_length']})")
                print(f"     é¢„è§ˆ: {div_info['text_preview']}...")
                print()
            
            # 3. æŸ¥æ‰¾ç‰¹å®šçš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            selectors_to_test = [
                '.rm_txt_con',
                '.main',
                '.content',
                '.article-content',
                '#content',
                '.article-body',
                '.post-content',
                '.entry-content',
                '.text-content',
                '.main-content',
                '.body-content',
                '.article-text',
                '.content-text',
                '.main-text',
                '.body-text',
                '.article',
                '.post',
                '.entry',
                '.text',
                '.body',
                '.main'
            ]
            
            print("æµ‹è¯•é€‰æ‹©å™¨:")
            working_selectors = []
            for selector in selectors_to_test:
                try:
                    elements = soup.select(selector)
                    if elements:
                        total_text = sum(len(elem.get_text(strip=True)) for elem in elements)
                        if total_text > 100:
                            working_selectors.append({
                                'selector': selector,
                                'count': len(elements),
                                'text_length': total_text
                            })
                            print(f"  âœ… {selector}: {len(elements)} ä¸ªå…ƒç´ , {total_text} å­—ç¬¦")
                        else:
                            print(f"  âš ï¸ {selector}: {len(elements)} ä¸ªå…ƒç´ , å†…å®¹å¤ªå°‘")
                    else:
                        print(f"  âŒ {selector}: æœªæ‰¾åˆ°å…ƒç´ ")
                except Exception as e:
                    print(f"  âŒ {selector}: é”™è¯¯ - {e}")
            
            # 4. æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„å…ƒç´ 
            print("\næŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å…ƒç´ :")
            keywords = ['ä¹ è¿‘å¹³', 'ä¸­å›½', 'å‘å±•', 'ç»æµ', 'æ”¿æ²»', 'ç¤¾ä¼š', 'å»ºè®¾', 'æ”¹é©']
            keyword_elements = []
            
            for div in divs:
                text = div.get_text(strip=True)
                if len(text) > 200:
                    for keyword in keywords:
                        if keyword in text:
                            class_name = ' '.join(div.get('class', []))
                            keyword_elements.append({
                                'element': div,
                                'keyword': keyword,
                                'text_length': len(text),
                                'class': class_name,
                                'text_preview': text[:100]
                            })
                            break
            
            if keyword_elements:
                print(f"æ‰¾åˆ° {len(keyword_elements)} ä¸ªåŒ…å«å…³é”®è¯çš„å…ƒç´ :")
                for i, elem_info in enumerate(keyword_elements[:3]):
                    print(f"  {i+1}. å…³é”®è¯: {elem_info['keyword']}")
                    print(f"     class: {elem_info['class']}")
                    print(f"     é•¿åº¦: {elem_info['text_length']}")
                    print(f"     é¢„è§ˆ: {elem_info['text_preview']}...")
                    print()
            
            return working_selectors, keyword_elements
            
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        return [], []


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆ†æäººæ°‘ç½‘é¡µé¢ç»“æ„...")
    print("=" * 60)
    
    # æµ‹è¯•å‡ ä¸ªä¸åŒçš„æ–‡ç« é¡µé¢
    test_urls = [
        "http://politics.people.com.cn/n1/2025/0908/c1024-40559264.html",
        "http://world.people.com.cn/n1/2025/0908/c1002-40559255.html",
        "http://health.people.com.cn/n1/2025/0908/c14739-40559156.html"
    ]
    
    all_selectors = []
    all_keyword_elements = []
    
    for url in test_urls:
        selectors, keyword_elements = analyze_page_structure(url)
        all_selectors.extend(selectors)
        all_keyword_elements.extend(keyword_elements)
        print()
    
    # æ±‡æ€»ç»“æœ
    print("ğŸ“Š æ±‡æ€»åˆ†æç»“æœ:")
    print("=" * 50)
    
    # ç»Ÿè®¡é€‰æ‹©å™¨
    selector_stats = {}
    for selector_info in all_selectors:
        selector = selector_info['selector']
        if selector not in selector_stats:
            selector_stats[selector] = {
                'count': 0,
                'total_text': 0,
                'avg_text': 0
            }
        selector_stats[selector]['count'] += 1
        selector_stats[selector]['total_text'] += selector_info['text_length']
    
    # è®¡ç®—å¹³å‡æ–‡æœ¬é•¿åº¦
    for selector in selector_stats:
        stats = selector_stats[selector]
        stats['avg_text'] = stats['total_text'] / stats['count']
    
    # æŒ‰å¹³å‡æ–‡æœ¬é•¿åº¦æ’åº
    sorted_selectors = sorted(selector_stats.items(), key=lambda x: x[1]['avg_text'], reverse=True)
    
    print("æ¨èçš„é€‰æ‹©å™¨ (æŒ‰å¹³å‡æ–‡æœ¬é•¿åº¦æ’åº):")
    for selector, stats in sorted_selectors[:10]:
        print(f"  {selector}: å¹³å‡ {stats['avg_text']:.0f} å­—ç¬¦, å‡ºç° {stats['count']} æ¬¡")
    
    print(f"\næ‰¾åˆ° {len(all_keyword_elements)} ä¸ªåŒ…å«å…³é”®è¯çš„å…ƒç´ ")
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)
    print("ğŸ’¡ å»ºè®®:")
    print("   1. ä½¿ç”¨æ¨èçš„é€‰æ‹©å™¨æ›´æ–°crawler")
    print("   2. æ·»åŠ å…³é”®è¯åŒ¹é…ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
    print("   3. æµ‹è¯•æ–°çš„é€‰æ‹©å™¨æ˜¯å¦æœ‰æ•ˆ")


if __name__ == '__main__':
    main()
