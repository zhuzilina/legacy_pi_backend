#!/usr/bin/env python3
"""
ä¿®å¤crawleråº”ç”¨ç¬¬äºŒå¤©æ— æ³•è·å–æ–°æ•°æ®çš„é—®é¢˜
"""

import os
import sys
import django
from datetime import datetime, timedelta
from django.utils import timezone

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'legacy_pi_backend.settings')
django.setup()

from crawler.redis_service import redis_service
from crawler.redis_models import RedisNewsArticle, RedisCrawlTask, RedisStats


def diagnose_crawler_issue():
    """è¯Šæ–­crawleré—®é¢˜"""
    print("ğŸ” è¯Šæ–­crawleråº”ç”¨é—®é¢˜...")
    print("=" * 50)
    
    # 1. æ£€æŸ¥Redisè¿æ¥
    print("1. æ£€æŸ¥Redisè¿æ¥:")
    if redis_service.test_connection():
        print("âœ… Redisè¿æ¥æ­£å¸¸")
    else:
        print("âŒ Redisè¿æ¥å¤±è´¥")
        return False
    print()
    
    # 2. æ£€æŸ¥ä»Šæ—¥çŠ¶æ€
    print("2. æ£€æŸ¥ä»Šæ—¥çˆ¬å–çŠ¶æ€:")
    today = timezone.now().date()
    today_str = today.isoformat()
    status = redis_service.get_daily_crawl_status()
    print(f"   ä»Šæ—¥æ—¥æœŸ: {today_str}")
    print(f"   ä»Šæ—¥çŠ¶æ€: {status}")
    print()
    
    # 3. æ£€æŸ¥ä»Šæ—¥æ–‡ç« 
    print("3. æ£€æŸ¥ä»Šæ—¥æ–‡ç« :")
    today_articles = redis_service.get_daily_articles()
    print(f"   ä»Šæ—¥æ–‡ç« æ•°é‡: {len(today_articles)}")
    if today_articles:
        print(f"   æ–‡ç« IDåˆ—è¡¨: {today_articles[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
    print()
    
    # 4. æ£€æŸ¥æ˜¨æ—¥æ•°æ®
    print("4. æ£€æŸ¥æ˜¨æ—¥æ•°æ®:")
    yesterday = today - timedelta(days=1)
    yesterday_str = yesterday.isoformat()
    yesterday_articles = redis_service.get_daily_articles(yesterday_str)
    print(f"   æ˜¨æ—¥æ—¥æœŸ: {yesterday_str}")
    print(f"   æ˜¨æ—¥æ–‡ç« æ•°é‡: {len(yesterday_articles)}")
    print()
    
    # 5. æ£€æŸ¥æ‰€æœ‰æ–‡ç« æ•°æ®
    print("5. æ£€æŸ¥æ‰€æœ‰æ–‡ç« æ•°æ®:")
    all_articles = RedisNewsArticle.filter()
    print(f"   æ€»æ–‡ç« æ•°é‡: {len(all_articles)}")
    
    # æŒ‰çŠ¶æ€åˆ†ç»„
    status_count = {}
    for article in all_articles:
        status = article.crawl_status
        status_count[status] = status_count.get(status, 0) + 1
    
    print("   æŒ‰çŠ¶æ€åˆ†ç»„:")
    for status, count in status_count.items():
        print(f"     {status}: {count}")
    print()
    
    # 6. æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
    print("6. æ£€æŸ¥ä»»åŠ¡çŠ¶æ€:")
    try:
        # è·å–æœ€è¿‘çš„ä»»åŠ¡
        all_tasks = RedisCrawlTask.filter()
        print(f"   æ€»ä»»åŠ¡æ•°é‡: {len(all_tasks)}")
        
        if all_tasks:
            recent_task = all_tasks[0]  # å‡è®¾æŒ‰æ—¶é—´æ’åº
            print(f"   æœ€è¿‘ä»»åŠ¡: {recent_task.task_name}")
            print(f"   ä»»åŠ¡çŠ¶æ€: {recent_task.status}")
            print(f"   åˆ›å»ºæ—¶é—´: {recent_task.created_at}")
    except Exception as e:
        print(f"   âŒ è·å–ä»»åŠ¡å¤±è´¥: {e}")
    print()
    
    return True


def fix_crawler_issue():
    """ä¿®å¤crawleré—®é¢˜"""
    print("ğŸ› ï¸ ä¿®å¤crawleråº”ç”¨é—®é¢˜...")
    print("=" * 50)
    
    today = timezone.now().date()
    today_str = today.isoformat()
    
    # 1. æ¸…ç†æ—§çŠ¶æ€
    print("1. æ¸…ç†æ—§çŠ¶æ€:")
    try:
        # æ¸…ç†æ˜¨å¤©çš„çŠ¶æ€
        yesterday = today - timedelta(days=1)
        yesterday_str = yesterday.isoformat()
        
        # åˆ é™¤æ˜¨å¤©çš„çŠ¶æ€é”®
        yesterday_status_key = f"crawl_status:{yesterday_str}"
        redis_service.redis_client.delete(yesterday_status_key)
        print(f"   âœ… æ¸…ç†äº†æ˜¨å¤©çš„çŠ¶æ€: {yesterday_status_key}")
        
        # æ¸…ç†æ›´æ—©çš„çŠ¶æ€
        for days_ago in range(2, 7):  # æ¸…ç†2-6å¤©å‰çš„çŠ¶æ€
            old_date = today - timedelta(days=days_ago)
            old_date_str = old_date.isoformat()
            old_status_key = f"crawl_status:{old_date_str}"
            redis_service.redis_client.delete(old_status_key)
        
        print("   âœ… æ¸…ç†äº†æ›´æ—©çš„çŠ¶æ€")
    except Exception as e:
        print(f"   âŒ æ¸…ç†çŠ¶æ€å¤±è´¥: {e}")
    print()
    
    # 2. é‡ç½®ä»Šæ—¥çŠ¶æ€
    print("2. é‡ç½®ä»Šæ—¥çŠ¶æ€:")
    try:
        # åˆ é™¤ä»Šæ—¥çŠ¶æ€
        today_status_key = f"crawl_status:{today_str}"
        redis_service.redis_client.delete(today_status_key)
        print(f"   âœ… é‡ç½®äº†ä»Šæ—¥çŠ¶æ€: {today_status_key}")
        
        # åˆ é™¤ä»Šæ—¥é”
        today_lock_key = f"daily_crawl_lock:{today_str}"
        redis_service.redis_client.delete(today_lock_key)
        print(f"   âœ… é‡ç½®äº†ä»Šæ—¥é”: {today_lock_key}")
    except Exception as e:
        print(f"   âŒ é‡ç½®çŠ¶æ€å¤±è´¥: {e}")
    print()
    
    # 3. æ¸…ç†æ—§æ–‡ç« æ•°æ®
    print("3. æ¸…ç†æ—§æ–‡ç« æ•°æ®:")
    try:
        # æ¸…ç†2å¤©å‰çš„æ•°æ®
        deleted_count = RedisStats.clear_old_data(days_to_keep=2)
        print(f"   âœ… æ¸…ç†äº† {deleted_count} ç¯‡æ—§æ–‡ç« ")
    except Exception as e:
        print(f"   âŒ æ¸…ç†æ–‡ç« å¤±è´¥: {e}")
    print()
    
    # 4. éªŒè¯ä¿®å¤ç»“æœ
    print("4. éªŒè¯ä¿®å¤ç»“æœ:")
    try:
        # æ£€æŸ¥ä»Šæ—¥çŠ¶æ€
        status = redis_service.get_daily_crawl_status()
        print(f"   ä»Šæ—¥çŠ¶æ€: {status} (åº”è¯¥ä¸ºNone)")
        
        # æ£€æŸ¥ä»Šæ—¥æ–‡ç« 
        today_articles = redis_service.get_daily_articles()
        print(f"   ä»Šæ—¥æ–‡ç« æ•°é‡: {len(today_articles)}")
        
        print("   âœ… ä¿®å¤å®Œæˆï¼Œç³»ç»Ÿå·²é‡ç½®")
    except Exception as e:
        print(f"   âŒ éªŒè¯å¤±è´¥: {e}")
    print()


def test_crawler_api():
    """æµ‹è¯•crawler API"""
    print("ğŸ§ª æµ‹è¯•crawler API...")
    print("=" * 50)
    
    import requests
    
    try:
        # æµ‹è¯•API
        response = requests.get("http://localhost/api/crawler/daily-articles/")
        print(f"APIå“åº”çŠ¶æ€: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"å“åº”æ•°æ®: {data}")
            
            if data.get('status') == 'crawling':
                print("âœ… APIæ­£å¸¸ï¼Œçˆ¬å–ä»»åŠ¡å·²å¯åŠ¨")
            elif data.get('status') == 'cached':
                print("âœ… APIæ­£å¸¸ï¼Œè¿”å›ç¼“å­˜æ•°æ®")
            else:
                print(f"âš ï¸ APIå“åº”å¼‚å¸¸: {data}")
        else:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.text}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•APIå¤±è´¥: {e}")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿®å¤crawleråº”ç”¨é—®é¢˜...")
    print("=" * 60)
    
    # 1. è¯Šæ–­é—®é¢˜
    if not diagnose_crawler_issue():
        print("âŒ è¯Šæ–­å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    print()
    
    # 2. ä¿®å¤é—®é¢˜
    fix_crawler_issue()
    
    print()
    
    # 3. æµ‹è¯•API
    test_crawler_api()
    
    print("âœ… ä¿®å¤å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“‹ ä¿®å¤æ€»ç»“:")
    print("   - æ¸…ç†äº†æ—§çš„çŠ¶æ€å’Œé”")
    print("   - é‡ç½®äº†ä»Šæ—¥çŠ¶æ€")
    print("   - æ¸…ç†äº†æ—§çš„æ–‡ç« æ•°æ®")
    print("   - ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œæ–°çš„çˆ¬å–")
    print()
    print("ğŸ’¡ ç°åœ¨å¯ä»¥é‡æ–°æµ‹è¯•crawler API:")
    print("   curl http://localhost/api/crawler/daily-articles/")


if __name__ == '__main__':
    main()
